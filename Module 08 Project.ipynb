{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM2msZP1lclm/e2mzh287gv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yazhinil/FMML20230017/blob/main/Module%2008%20Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install black"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ztx7onWdDwcw",
        "outputId": "bcec26e3-b9fd-4b0b-aa56-16df889514b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting black\n",
            "  Downloading black-24.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from black) (8.1.7)\n",
            "Collecting mypy-extensions>=0.4.3 (from black)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Requirement already satisfied: packaging>=22.0 in /usr/local/lib/python3.10/dist-packages (from black) (24.0)\n",
            "Collecting pathspec>=0.9.0 (from black)\n",
            "  Downloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.10/dist-packages (from black) (4.2.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from black) (2.0.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from black) (4.11.0)\n",
            "Installing collected packages: pathspec, mypy-extensions, black\n",
            "Successfully installed black-24.4.0 mypy-extensions-1.0.0 pathspec-0.12.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install mypy\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lk4DcqzgECSf",
        "outputId": "476aba3b-b5be-4ca8-de87-29e3cc1d332b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mypy\n",
            "  Downloading mypy-1.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.5/12.5 MB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from mypy) (4.11.0)\n",
            "Requirement already satisfied: mypy-extensions>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from mypy) (1.0.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from mypy) (2.0.1)\n",
            "Installing collected packages: mypy\n",
            "Successfully installed mypy-1.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install PyPDF2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pb4LOwNMF3_S",
        "outputId": "5ff758ea-5177-4fa6-8dc0-713cf6c26ec4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pylint\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KnbrNah_EEMi",
        "outputId": "f53b575a-8ad5-4431-c4ff-f3a28da582dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pylint\n",
            "  Downloading pylint-3.1.0-py3-none-any.whl (515 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m515.6/515.6 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: platformdirs>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from pylint) (4.2.0)\n",
            "Collecting astroid<=3.2.0-dev0,>=3.1.0 (from pylint)\n",
            "  Downloading astroid-3.1.0-py3-none-any.whl (275 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m275.6/275.6 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting isort!=5.13.0,<6,>=4.2.5 (from pylint)\n",
            "  Downloading isort-5.13.2-py3-none-any.whl (92 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.3/92.3 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mccabe<0.8,>=0.6 (from pylint)\n",
            "  Downloading mccabe-0.7.0-py2.py3-none-any.whl (7.3 kB)\n",
            "Collecting tomlkit>=0.10.1 (from pylint)\n",
            "  Downloading tomlkit-0.12.4-py3-none-any.whl (37 kB)\n",
            "Collecting dill>=0.2 (from pylint)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from pylint) (2.0.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from astroid<=3.2.0-dev0,>=3.1.0->pylint) (4.11.0)\n",
            "Installing collected packages: tomlkit, mccabe, isort, dill, astroid, pylint\n",
            "Successfully installed astroid-3.1.0 dill-0.3.8 isort-5.13.2 mccabe-0.7.0 pylint-3.1.0 tomlkit-0.12.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pytest\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Bjc4A-aEGn_",
        "outputId": "7f54a21f-48ba-4915-9580-8dda26d72d09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytest in /usr/local/lib/python3.10/dist-packages (7.4.4)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest) (2.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pytest) (24.0)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest) (1.4.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest) (1.2.1)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest) (2.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sphinx\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Du5tpunnEILc",
        "outputId": "9aa1bef9-173c-4616-baf3-63c719e733c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sphinx in /usr/local/lib/python3.10/dist-packages (5.0.2)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp in /usr/local/lib/python3.10/dist-packages (from sphinx) (1.0.8)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp in /usr/local/lib/python3.10/dist-packages (from sphinx) (1.0.6)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath in /usr/local/lib/python3.10/dist-packages (from sphinx) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from sphinx) (2.0.5)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from sphinx) (1.1.10)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp in /usr/local/lib/python3.10/dist-packages (from sphinx) (1.0.7)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.10/dist-packages (from sphinx) (3.1.3)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.10/dist-packages (from sphinx) (2.16.1)\n",
            "Requirement already satisfied: docutils<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from sphinx) (0.18.1)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.10/dist-packages (from sphinx) (2.2.0)\n",
            "Requirement already satisfied: babel>=1.3 in /usr/local/lib/python3.10/dist-packages (from sphinx) (2.14.0)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.10/dist-packages (from sphinx) (0.7.16)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.10/dist-packages (from sphinx) (1.4.1)\n",
            "Requirement already satisfied: requests>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from sphinx) (2.31.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from sphinx) (24.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=2.3->sphinx) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sphinx-rtd-theme"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hThRwB6REJ9P",
        "outputId": "7ee11c99-96e1-4806-e945-04d357a4d5b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sphinx-rtd-theme in /usr/local/lib/python3.10/dist-packages (2.0.0)\n",
            "Requirement already satisfied: sphinx<8,>=5 in /usr/local/lib/python3.10/dist-packages (from sphinx-rtd-theme) (5.0.2)\n",
            "Requirement already satisfied: docutils<0.21 in /usr/local/lib/python3.10/dist-packages (from sphinx-rtd-theme) (0.18.1)\n",
            "Requirement already satisfied: sphinxcontrib-jquery<5,>=4 in /usr/local/lib/python3.10/dist-packages (from sphinx-rtd-theme) (4.1)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp in /usr/local/lib/python3.10/dist-packages (from sphinx<8,>=5->sphinx-rtd-theme) (1.0.8)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp in /usr/local/lib/python3.10/dist-packages (from sphinx<8,>=5->sphinx-rtd-theme) (1.0.6)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath in /usr/local/lib/python3.10/dist-packages (from sphinx<8,>=5->sphinx-rtd-theme) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from sphinx<8,>=5->sphinx-rtd-theme) (2.0.5)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from sphinx<8,>=5->sphinx-rtd-theme) (1.1.10)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp in /usr/local/lib/python3.10/dist-packages (from sphinx<8,>=5->sphinx-rtd-theme) (1.0.7)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.10/dist-packages (from sphinx<8,>=5->sphinx-rtd-theme) (3.1.3)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.10/dist-packages (from sphinx<8,>=5->sphinx-rtd-theme) (2.16.1)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.10/dist-packages (from sphinx<8,>=5->sphinx-rtd-theme) (2.2.0)\n",
            "Requirement already satisfied: babel>=1.3 in /usr/local/lib/python3.10/dist-packages (from sphinx<8,>=5->sphinx-rtd-theme) (2.14.0)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.10/dist-packages (from sphinx<8,>=5->sphinx-rtd-theme) (0.7.16)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.10/dist-packages (from sphinx<8,>=5->sphinx-rtd-theme) (1.4.1)\n",
            "Requirement already satisfied: requests>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from sphinx<8,>=5->sphinx-rtd-theme) (2.31.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from sphinx<8,>=5->sphinx-rtd-theme) (24.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=2.3->sphinx<8,>=5->sphinx-rtd-theme) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx<8,>=5->sphinx-rtd-theme) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx<8,>=5->sphinx-rtd-theme) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx<8,>=5->sphinx-rtd-theme) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx<8,>=5->sphinx-rtd-theme) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import black\n",
        "import mypy\n",
        "import pylint\n",
        "import pytest\n",
        "import sphinx"
      ],
      "metadata": {
        "id": "qWRBpZ5QIhRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from os import stat\n",
        "import typing\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import typing\n",
        "\n",
        "\n",
        "class BagOfWords:\n",
        "    \"\"\"\n",
        "    A type of encoder, makes\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, text: typing.Iterable) -> None:\n",
        "        \"\"\"\n",
        "        Generate the bag of words\n",
        "        :param data: an array of words, or an iterable containing arrays of words\n",
        "        \"\"\"\n",
        "        text = np.array(self.__linearize_array('resume-dataset.csv'))\n",
        "        self.index_to_words = np.unique('resume-dataset.csv')\n",
        "        self.words_to_index = {w: i for i, w in enumerate(self.index_to_words)}\n",
        "\n",
        "    @classmethod\n",
        "    def __linearize_array(cls, text):\n",
        "        x = []\n",
        "        for item in text:\n",
        "            if isinstance(item, str):\n",
        "                x.append(item)\n",
        "            else:\n",
        "                x.extend(cls.__linearize_array(item))\n",
        "        return x\n",
        "\n",
        "    def __call__(self, text: typing.Iterable[str]) -> np.array:\n",
        "        return self.get_counts(text)\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.index_to_words)\n",
        "\n",
        "    def encode_data(\n",
        "        self: \"BagOfWords\",\n",
        "        text: typing.Union[typing.Iterable[str], typing.Iterable[typing.Iterable[str]]],\n",
        "    ) -> np.array:\n",
        "        \"\"\"\n",
        "        Compute the encodings of words in a new input tokenized string\n",
        "        \"\"\"\n",
        "        x = []\n",
        "        for item in text:\n",
        "            if isinstance(item, str):\n",
        "                if item in self.words_to_index:\n",
        "                    x.append(self.words_to_index[item])\n",
        "            else:\n",
        "                x.append(self.encode_data(item))\n",
        "        return x\n",
        "\n",
        "    def decode_data(self: \"BagOfWords\", encoded_text: typing.Iterable[int]):\n",
        "        if isinstance(encoded_text, int) or isinstance(encoded_text, np.int64):\n",
        "            return self.index_to_words[encoded_text]\n",
        "        else:\n",
        "            return list(map(self.decode_data, encoded_text))\n",
        "\n",
        "    def get_counts(\n",
        "        self: \"BagOfWords\",\n",
        "        text: typing.Union[typing.Iterable[str], typing.Iterable[typing.Iterable[str]]],\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Computes the counts of words in a new input tokenized string\n",
        "        \"\"\"\n",
        "        if len(text) == 0 or isinstance(text[0], str):\n",
        "            x = np.zeros(shape=len(self))\n",
        "            for word in text:\n",
        "                if word in self.words_to_index:\n",
        "                    x[self.words_to_index[word]] += 1\n",
        "            return x\n",
        "        else:\n",
        "            return np.stack([self.get_counts(item) for item in text], axis=0)\n"
      ],
      "metadata": {
        "id": "Lr3sEIC5TO5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "class LabelEncoder:\n",
        "    \"\"\"\n",
        "    Label encode a series of labels\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data) -> None:\n",
        "        self.__training_data = data\n",
        "        self.index_to_token = list(set(data))\n",
        "        self.token_to_index = {\n",
        "            token: index for index, token in enumerate(self.index_to_token)\n",
        "        }\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.token_to_index)\n",
        "\n",
        "    @property\n",
        "    def encoded_data(self):\n",
        "        return np.array([self.token_to_index[token] for token in self.__training_data])\n",
        "\n",
        "    def encode(self, data):\n",
        "        return np.array([self.token_to_index[token] for token in data])\n",
        "\n",
        "    def decode(self, data):\n",
        "        if isinstance(data, int) or isinstance(data, np.int64):\n",
        "            return self.index_to_token[data]\n",
        "        else:\n",
        "            return np.array([self.index_to_token[index] for index in data])\n"
      ],
      "metadata": {
        "id": "R5gy-Xj8TULW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import typing\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class BayesianMulticlassModel:\n",
        "    \"\"\"\n",
        "    A multi-class bayesian classfier from encoded text tokens\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_classes, num_tokens) -> None:\n",
        "        self.counts = np.zeros(shape=(num_classes, num_tokens))\n",
        "\n",
        "    def fit(self, x_train: typing.Iterable[np.ndarray], y_train: typing.Iterable[int]):\n",
        "        for x, y in zip(x_train, y_train):\n",
        "             self.counts[y] += x\n",
        "\n",
        "    def predict(self, counts_vector):\n",
        "        class_frequencies = np.sum(self.counts, axis=1)\n",
        "        word_frequencies = np.sum(self.counts, axis=0)\n",
        "\n",
        "        prior = class_frequencies / np.sum(class_frequencies)  # p(label)\n",
        "        likelihood = self.counts / np.expand_dims(\n",
        "            class_frequencies, axis=1\n",
        "        )  # p(word|label)\n",
        "        evidence = word_frequencies / np.sum(word_frequencies)  # p(word)\n",
        "\n",
        "        likelihood = np.multiply(likelihood, counts_vector)\n",
        "        prior = np.expand_dims(prior, axis=1)\n",
        "\n",
        "        posterior_marginal = prior * likelihood / evidence + 0.00001\n",
        "        posterior_joint = np.sum(np.log(posterior_marginal), axis=1)\n",
        "        return np.flip(np.argsort(posterior_joint))\n"
      ],
      "metadata": {
        "id": "0S4H3RuHTx7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "class BayesianModelExplainer(BayesianMulticlassModel):\n",
        "    \"\"\"\n",
        "    Explainer of the decision made by the base model\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, label_encoder: LabelEncoder, bag_of_words: BagOfWords) -> None:\n",
        "        super().__init__(len(label_encoder), len(bag_of_words))\n",
        "        self.bag_of_words = bag_of_words\n",
        "        self.label_encoder = label_encoder\n",
        "\n",
        "    def explain(self, text=None, label_filter=None):\n",
        "        \"\"\"\n",
        "        Visualize what are the prior probabilities of classes and which words\n",
        "        add the the likelihood of each class.\n",
        "        \"\"\"\n",
        "        class_frequencies = np.sum(self.counts, axis=1)\n",
        "        word_frequencies = np.sum(self.counts, axis=0)\n",
        "\n",
        "        prior = class_frequencies / np.sum(class_frequencies)  # p(label)\n",
        "        likelihood = self.counts / np.expand_dims(\n",
        "            class_frequencies, axis=1\n",
        "        )  # p(word|label)\n",
        "        evidence = word_frequencies / np.sum(word_frequencies)  # p(word)\n",
        "\n",
        "        if text is not None:\n",
        "            counts_vector = self.bag_of_words.get_counts(text)\n",
        "            likelihood = np.multiply(likelihood, counts_vector)\n",
        "\n",
        "        prior_ordering = np.flip(np.argsort(prior))\n",
        "        for item in prior_ordering:\n",
        "            likelihood = likelihood / (evidence + 0.00001)\n",
        "            label = self.label_encoder.decode(item)\n",
        "            word_ids = np.flip(np.argsort(likelihood[item]))\n",
        "            word_ids = word_ids[:10]\n",
        "            if label_filter is None or label in label_filter:\n",
        "                print(f\"{label}: {' '.join(self.bag_of_words.decode_data(word_ids))}\")\n"
      ],
      "metadata": {
        "id": "aHMcg9b-VTk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class BayesianModelExplainer(BayesianMulticlassModel):\n",
        "    \"\"\"\n",
        "    Explainer of the decision made by the base model\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, label_encoder: LabelEncoder, bag_of_words: BagOfWords) -> None:\n",
        "        super().__init__(len(label_encoder), len(bag_of_words))\n",
        "        self.bag_of_words = bag_of_words\n",
        "        self.label_encoder = label_encoder\n",
        "\n",
        "    def explain(self, text=None, label_filter=None):\n",
        "        \"\"\"\n",
        "        Visualize what are the prior probabilities of classes and which words\n",
        "        add the the likelihood of each class.\n",
        "        \"\"\"\n",
        "        class_frequencies = np.sum(self.counts, axis=1)\n",
        "        word_frequencies = np.sum(self.counts, axis=0)\n",
        "\n",
        "        prior = class_frequencies / np.sum(class_frequencies)  # p(label)\n",
        "        likelihood = self.counts / np.expand_dims(\n",
        "            class_frequencies, axis=1\n",
        "        )  # p(word|label)\n",
        "        evidence = word_frequencies / np.sum(word_frequencies)  # p(word)\n",
        "\n",
        "        if text is not None:\n",
        "            counts_vector = self.bag_of_words.get_counts(text)\n",
        "            likelihood = np.multiply(likelihood, counts_vector)\n",
        "\n",
        "        prior_ordering = np.flip(np.argsort(prior))\n",
        "        for item in prior_ordering:\n",
        "            likelihood = likelihood / (evidence + 0.00001)\n",
        "            label = self.label_encoder.decode(item)\n",
        "            word_ids = np.flip(np.argsort(likelihood[item]))\n",
        "            word_ids = word_ids[:10]\n",
        "            if label_filter is None or label in label_filter:\n",
        "                print(f\"{label}: {' '.join(self.bag_of_words.decode_data(word_ids))}\")\n"
      ],
      "metadata": {
        "id": "gzf1AxoQbPPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "WORD_LENGTH_THRESHOLD = 2\n",
        "WORD_COUNT_THRESHOLD = 1"
      ],
      "metadata": {
        "id": "UeuD5xQ7bV2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from collections import defaultdict\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import PyPDF2\n",
        "from PyPDF2 import PdfReader\n",
        "\n",
        "\n",
        "def clean_text(text: str):\n",
        "    \"\"\"\n",
        "    Given text it removes all the non-character words, small words,\n",
        "    converts everything to small letters, tokenizes and returns as a list.\n",
        "    :param text: The text to be cleaned\n",
        "    \"\"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(\"[^a-z]\", \" \", text)\n",
        "    data = text.split()\n",
        "    data = list(filter(lambda x: len(x) >= WORD_LENGTH_THRESHOLD, data))\n",
        "    return data\n",
        "\n",
        "\n",
        "def parse_pdf(filename: str):\n",
        "    \"\"\"\n",
        "    Read text from a PDF file.\n",
        "    Clean the text, tokenize it, and return as a list of tokens.\n",
        "    :param filename: The path to the PDF file.\n",
        "    \"\"\"\n",
        "    with open('computers_2.pdf', 'rb') as file:\n",
        "        pdf_reader = PdfReader(file)\n",
        "        file_contents = \"\"\n",
        "        for page_num in range(pdf_reader.numPages):\n",
        "            page = pdf_reader.getPage(page_num)\n",
        "            page_text = page.extractText()\n",
        "            file_contents += page_text\n",
        "\n",
        "    # Clean the extracted text and return as a list of tokens\n",
        "    return clean_text(file_contents)\n",
        "\n",
        "def clean_text(text: str):\n",
        "    \"\"\"\n",
        "    Clean the text by removing unwanted characters, punctuation, etc.\n",
        "    Tokenize the text and return as a list of tokens.\n",
        "    \"\"\"\n",
        "    # Implement your text cleaning and tokenization logic here\n",
        "    pass\n",
        "\n",
        "\n",
        "\n",
        "def parse_resume_df():\n",
        "    resume_df = pd.read_csv(\"resume-dataset.csv\")\n",
        "    resume_df[\"Keywords\"] = resume_df[\"Resume\"].apply(clean_text)\n",
        "    return resume_df[\"Keywords\"].values, resume_df[\"Category\"].values\n",
        "\n"
      ],
      "metadata": {
        "id": "ljt_DuEUbY8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    x_train, y_train = parse_resume_df()\n",
        "    bag_of_words = BagOfWords(x_train)\n",
        "    label_encoder = LabelEncoder(y_train)\n",
        "\n",
        "    x_train = bag_of_words.get_counts(x_train)\n",
        "    y_train = label_encoder.encode(y_train)\n",
        "    model = BayesianMulticlassModel(len(label_encoder), len(bag_of_words))\n",
        "    model.fit(x_train=x_train, y_train=y_train)\n",
        "\n",
        "    x_test_input = parse_pdf(\"computers_2.pdf\")\n",
        "    x_test = bag_of_words.get_counts(x_test_input)\n",
        "    result = model.predict(x_train[0])\n",
        "    result = label_encoder.decode(result)\n",
        "\n",
        "    for job in result[:5]:\n",
        "        print(job)\n",
        "\n",
        "    explainable_model = BayesianModelExplainer(label_encoder, bag_of_words)\n",
        "    explainable_model.fit(x_train=x_train, y_train=y_train)\n",
        "\n",
        "    print(\n",
        "        \"\"\"\n",
        "ANALYSIS OF TRAINED PRIOR\n",
        "-------------------------\"\"\"\n",
        "    )\n",
        "    explainable_model.explain()\n",
        "\n",
        "    print(\n",
        "        \"\"\"\n",
        "ANALYSIS OF TRAINED EVIDENCE\n",
        "----------------------------\"\"\"\n",
        "    )\n",
        "    explainable_model.explain(x_test_input)"
      ],
      "metadata": {
        "id": "R_qnZRzl8xyC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}